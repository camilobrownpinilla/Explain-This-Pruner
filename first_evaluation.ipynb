{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruners.pruning_methods import Pruner, PruningMethod\n",
    "from pruners.Pruner_Test_Suite import *\n",
    "from pruners.Fine_Tuning import *\n",
    "import torch\n",
    "from evaluations.evaluator import Evaluator\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "Evaluated on Camilo CANNOT CODE FOR HIS LIFE. I DONT LIKE HIM!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal infedility: 16.697126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstructured Pruning Infidelity: 0.667052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Pruned infidelity: 12.824641\n",
      "********************************************************************************\n",
      "Evaluated on David is GREAT at soccer. Can recommend <thumbs up>!\n",
      "Normal infedility: 0.066186\n",
      "Unstructured Pruning Infidelity: 0.984423\n",
      "L1 Pruned infidelity: 0.044637\n",
      "********************************************************************************\n",
      "Evaluated on Joey is joey. I feel very neutrally about him\n",
      "Normal infedility: 0.476829\n",
      "Unstructured Pruning Infidelity: 0.529545\n",
      "L1 Pruned infidelity: 1.966267\n",
      "********************************************************************************\n",
      "Evaluated on Finale is the best professor Harvard has EVER had. Would recommend!\n",
      "Normal infedility: 0.152795\n",
      "Unstructured Pruning Infidelity: 0.455202\n",
      "L1 Pruned infidelity: 0.149887\n",
      "********************************************************************************\n",
      "Evaluated on I AM GOING TO SCREAMMMMMMMMMMM AHHHHHHHHHHHH\n",
      "Normal infedility: 25.010798\n",
      "Unstructured Pruning Infidelity: 0.009562\n",
      "L1 Pruned infidelity: 16.432804\n",
      "********************************************************************************\n",
      "Evaluated on Paula and Hiwot are great TFs!\n",
      "Normal infedility: 0.161426\n",
      "Unstructured Pruning Infidelity: 0.330403\n",
      "L1 Pruned infidelity: 0.137418\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from explainers.explanation_methods import SHAP, LIME\n",
    "import shap\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "print('model loaded')\n",
    "\n",
    "input = 'Hello, my dog is so terribly ugly'\n",
    "tokenized_input = tokenizer(input, return_tensors=\"pt\")\n",
    "label = torch.tensor([0]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "pruned_model = deepcopy(model)\n",
    "unstructured_model = deepcopy(model)\n",
    "\n",
    "l1_paras_to_prune = []\n",
    "random_paras_to_prune = []\n",
    "\n",
    "\n",
    "for layer in pruned_model.bert.encoder.layer:\n",
    "    # Attention weights (query, key, value, and output projection)\n",
    "    l1_paras_to_prune.append((layer.attention.self.query, 'weight'))\n",
    "    l1_paras_to_prune.append((layer.attention.self.key, 'weight'))\n",
    "    l1_paras_to_prune.append((layer.attention.self.value, 'weight'))\n",
    "    l1_paras_to_prune.append((layer.attention.output.dense, 'weight'))\n",
    "    \n",
    "    # Intermediate dense layer\n",
    "    l1_paras_to_prune.append((layer.intermediate.dense, 'weight'))\n",
    "    \n",
    "    # Output dense layer\n",
    "    l1_paras_to_prune.append((layer.output.dense, 'weight'))\n",
    "\n",
    "for layer in unstructured_model.bert.encoder.layer:\n",
    "    # Attention weights (query, key, value, and output projection)\n",
    "    random_paras_to_prune.append((layer.attention.self.query, 'weight'))\n",
    "    random_paras_to_prune.append((layer.attention.self.key, 'weight'))\n",
    "    random_paras_to_prune.append((layer.attention.self.value, 'weight'))\n",
    "    random_paras_to_prune.append((layer.attention.output.dense, 'weight'))\n",
    "    \n",
    "    # Intermediate dense layer\n",
    "    random_paras_to_prune.append((layer.intermediate.dense, 'weight'))\n",
    "    \n",
    "    # Output dense layer\n",
    "    random_paras_to_prune.append((layer.output.dense, 'weight'))\n",
    "\n",
    "# Initialize Pruner instance\n",
    "Bert_pruner = Pruner(pruned_model)\n",
    "Unstructured_pruner = Pruner(unstructured_model)\n",
    "\n",
    "# Initialize PruningMethod instances\n",
    "L1_prune_Bert = PruningMethod(type = \"L1Unstructured\", paras_to_prune = l1_paras_to_prune, percentage = 0.2, mask = None)\n",
    "Unstructured_prune = PruningMethod(type='RandomUnstructured', paras_to_prune= random_paras_to_prune, percentage=0.2, mask=None)\n",
    "\n",
    "# Prune the model\n",
    "Bert_pruner.prune(L1_prune_Bert)\n",
    "L1_prune_Bert.remover()\n",
    "\n",
    "Unstructured_pruner.prune(Unstructured_prune)\n",
    "Unstructured_prune.remover()\n",
    "\n",
    "pruned_shapExplainer = SHAP(pruned_model, tokenizer, device=device)\n",
    "unstructured_shapExplainer = SHAP(unstructured_model, tokenizer, device=device)\n",
    "normal_shapExplainer = SHAP(model, tokenizer, device=device)\n",
    "\n",
    "# pruned_explanation = pruned_shapExplainer.explain(input)\n",
    "# normal_explanation = normal_shapExplainer.explain(input)\n",
    "\n",
    "unstructured_evaluator = Evaluator(unstructured_shapExplainer)\n",
    "pruned_evaluator = Evaluator(pruned_shapExplainer)\n",
    "normal_evaluator = Evaluator(normal_shapExplainer)\n",
    "\n",
    "inputs = ['Camilo CANNOT CODE FOR HIS LIFE. I DONT LIKE HIM!!!!',\n",
    "          'David is GREAT at soccer. Can recommend <thumbs up>!',\n",
    "          'Joey is joey. I feel very neutrally about him',\n",
    "          'Finale is the best professor Harvard has EVER had. Would recommend!',\n",
    "          'I AM GOING TO SCREAMMMMMMMMMMM AHHHHHHHHHHHH',\n",
    "          'Paula and Hiwot are great TFs!']\n",
    "\n",
    "for input in inputs:\n",
    "    print(f'Evaluated on {input}')\n",
    "    print(f'Normal infedility: {normal_evaluator.get_local_infidelity(input):4f}')\n",
    "    print(f'Unstructured Pruning Infidelity: {unstructured_evaluator.get_local_infidelity(input):4f}')\n",
    "    print(f'L1 Pruned infidelity: {pruned_evaluator.get_local_infidelity(input):4f}')\n",
    "    print('*' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2822",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
