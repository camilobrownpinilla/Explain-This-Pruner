{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilopinilla/anaconda3/envs/cs2822/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pruners.pruning_methods import L1Unstructured, RandUnstructured\n",
    "import torch\n",
    "from evaluations.evaluator import Evaluator\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from explainers.explanation_methods import SHAP, LIME\n",
    "import shap\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pruner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     random_paras_to_prune\u001b[38;5;241m.\u001b[39mappend((layer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mdense, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Initialize Pruner instance\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m Bert_pruner \u001b[38;5;241m=\u001b[39m \u001b[43mPruner\u001b[49m(pruned_model)\n\u001b[1;32m     46\u001b[0m Unstructured_pruner \u001b[38;5;241m=\u001b[39m Pruner(unstructured_model)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Initialize PruningMethod instances\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pruner' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "print('model loaded')\n",
    "\n",
    "input = 'Hello, my dog is so terribly ugly'\n",
    "tokenized_input = tokenizer(input, return_tensors=\"pt\")\n",
    "label = torch.tensor([0]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "pruned_model = deepcopy(model)\n",
    "unstructured_model = deepcopy(model)\n",
    "\n",
    "l1_paras_to_prune = []\n",
    "random_paras_to_prune = []\n",
    "\n",
    "\n",
    "for layer in pruned_model.bert.encoder.layer:\n",
    "    # Attention weights (query, key, value, and output projection)\n",
    "    l1_paras_to_prune.append((layer.attention.self.query, 'weight'))\n",
    "    l1_paras_to_prune.append((layer.attention.self.key, 'weight'))\n",
    "    l1_paras_to_prune.append((layer.attention.self.value, 'weight'))\n",
    "    l1_paras_to_prune.append((layer.attention.output.dense, 'weight'))\n",
    "    \n",
    "    # Intermediate dense layer\n",
    "    l1_paras_to_prune.append((layer.intermediate.dense, 'weight'))\n",
    "    \n",
    "    # Output dense layer\n",
    "    l1_paras_to_prune.append((layer.output.dense, 'weight'))\n",
    "\n",
    "for layer in unstructured_model.bert.encoder.layer:\n",
    "    # Attention weights (query, key, value, and output projection)\n",
    "    random_paras_to_prune.append((layer.attention.self.query, 'weight'))\n",
    "    random_paras_to_prune.append((layer.attention.self.key, 'weight'))\n",
    "    random_paras_to_prune.append((layer.attention.self.value, 'weight'))\n",
    "    random_paras_to_prune.append((layer.attention.output.dense, 'weight'))\n",
    "    \n",
    "    # Intermediate dense layer\n",
    "    random_paras_to_prune.append((layer.intermediate.dense, 'weight'))\n",
    "    \n",
    "    # Output dense layer\n",
    "    random_paras_to_prune.append((layer.output.dense, 'weight'))\n",
    "\n",
    "# Initialize Pruner instance\n",
    "Bert_pruner = Pruner(pruned_model)\n",
    "Unstructured_pruner = Pruner(unstructured_model)\n",
    "\n",
    "# Initialize PruningMethod instances\n",
    "L1_prune_Bert = PruningMethod(type = \"L1Unstructured\", paras_to_prune = l1_paras_to_prune, percentage = 0.2, mask = None)\n",
    "Unstructured_prune = PruningMethod(type='RandomUnstructured', paras_to_prune= random_paras_to_prune, percentage=0.2, mask=None)\n",
    "\n",
    "# Prune the model\n",
    "Bert_pruner.prune(L1_prune_Bert)\n",
    "L1_prune_Bert.remover()\n",
    "\n",
    "Unstructured_pruner.prune(Unstructured_prune)\n",
    "Unstructured_prune.remover()\n",
    "\n",
    "pruned_shapExplainer = SHAP(pruned_model, tokenizer, device=device)\n",
    "unstructured_shapExplainer = SHAP(unstructured_model, tokenizer, device=device)\n",
    "normal_shapExplainer = SHAP(model, tokenizer, device=device)\n",
    "\n",
    "# pruned_explanation = pruned_shapExplainer.explain(input)\n",
    "# normal_explanation = normal_shapExplainer.explain(input)\n",
    "\n",
    "unstructured_evaluator = Evaluator(unstructured_shapExplainer)\n",
    "pruned_evaluator = Evaluator(pruned_shapExplainer)\n",
    "normal_evaluator = Evaluator(normal_shapExplainer)\n",
    "\n",
    "inputs = ['Camilo CANNOT CODE FOR HIS LIFE. I DONT LIKE HIM!!!!',\n",
    "          'David is GREAT at soccer. Can recommend <thumbs up>!',\n",
    "          'Joey is joey. I feel very neutrally about him',\n",
    "          'Finale is the best professor Harvard has EVER had. Would recommend!',\n",
    "          'I AM GOING TO SCREAMMMMMMMMMMM AHHHHHHHHHHHH',\n",
    "          'Paula and Hiwot are great TFs!']\n",
    "\n",
    "for input in inputs:\n",
    "    print(f'Evaluated on {input}')\n",
    "    print(f'Normal infedility: {normal_evaluator.get_local_infidelity(input):4f}')\n",
    "    print(f'Unstructured Pruning Infidelity: {unstructured_evaluator.get_local_infidelity(input):4f}')\n",
    "    print(f'L1 Pruned infidelity: {pruned_evaluator.get_local_infidelity(input):4f}')\n",
    "    print('*' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_suite(model, tokenizer, inputs, prune_ptg):\n",
    "    # Init models and grab model params to prune\n",
    "    # TODO: Refactor this shi\n",
    "    model.to(device)\n",
    "    randunstrctured_model = deepcopy(model).to(device)\n",
    "    l1unstructured_model = deepcopy(model).to(device)\n",
    "\n",
    "    randunstruct_params = [param for name, param in randunstrctured_model.named_parameters() if 'embeddings' not in name]\n",
    "    l1unstruct_params = [param for name, param in l1unstructured_model.named_parameters() if 'embeddings' not in name]\n",
    "\n",
    "    # Initialize pruners and make pruned models\n",
    "    print('Pruning models...')\n",
    "    unpruned_model = model\n",
    "    RandUnstructured(randunstrctured_model, \n",
    "                    randunstruct_params,\n",
    "                    prune_ptg)\n",
    "    L1Unstructured(l1unstructured_model, \n",
    "                    l1unstruct_params,\n",
    "                    prune_ptg)\n",
    "    \n",
    "\n",
    "   # Init explainers\n",
    "    shap_randunstruct, lime_randunstruct = SHAP(randunstrctured_model, tokenizer, device),\\\n",
    "                                           LIME(randunstrctured_model, tokenizer, device)\n",
    "    shap_l1unstruct, lime_l1unstruct = SHAP(l1unstructured_model, tokenizer, device),\\\n",
    "                                       LIME(l1unstructured_model, tokenizer, device)\n",
    "    shap_unpruned, lime_unpruned = SHAP(unpruned_model, tokenizer, device),\\\n",
    "                                   LIME(unpruned_model, tokenizer, device)\n",
    "    \n",
    "    # Init evaluators\n",
    "    randunstruct_evaluators = {'shap': Evaluator(shap_randunstruct)}\n",
    "                               #'lime': Evaluator(lime_randunstruct)}\n",
    "    l1unstruct_evaluators = {'shap': Evaluator(shap_l1unstruct)}\n",
    "                            # 'lime': Evaluator(lime_l1unstruct)}\n",
    "    unrpuned_evaluators = {'shap': Evaluator(shap_unpruned)}\n",
    "                           #'lime': Evaluator(lime_unpruned)}\n",
    "\n",
    "    # Janky for now but hang with me\n",
    "    infidelities_ = {'unpruned': unrpuned_evaluators,\n",
    "                    'l1unstruct': l1unstruct_evaluators,\n",
    "                    'randunstruct': randunstruct_evaluators}\n",
    "    \n",
    "    # Run evaluations, storing in dictionary of \n",
    "    # {prune_method: \n",
    "    #   {explanation_method: infidelity}\n",
    "    # }\n",
    "    print('Evaluating explanations...')\n",
    "    infidelities = {}\n",
    "    for input in tqdm(inputs, desc='Evaluating', unit='input'):\n",
    "        for prune_method, evaluator_set in infidelities_.items():\n",
    "            # Initialize prune_method in infidelities if not present\n",
    "            if prune_method not in infidelities:\n",
    "                infidelities[prune_method] = {}  # Initialize prune_method dict\n",
    "            \n",
    "            for expla_method, evaluator in evaluator_set.items():\n",
    "                # Initialize expla_method as a list if not present\n",
    "                if expla_method not in infidelities[prune_method]:\n",
    "                    infidelities[prune_method][expla_method] = []\n",
    "                \n",
    "                # Append the result of get_local_infidelity to the list\n",
    "                infidelities[prune_method][expla_method].append(evaluator.get_local_infidelity(input))\n",
    "\n",
    "    return infidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning models...\n",
      "Evaluating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [01:07<00:00, 11.22s/input]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = model.to(device)\n",
    "# model_params = [param for name, param in model.named_parameters() if 'embeddings' not in name]\n",
    "\n",
    "# l1_model = deepcopy(model).to(device)\n",
    "# l1_params = [param for name, param in l1_model.named_parameters() if 'embeddings' not in name]\n",
    "\n",
    "# unstruct_model = deepcopy(model).to(device)\n",
    "# unstruct_params = [param for name, param in unstruct_model.named_parameters() if 'embeddings' not in name]\n",
    "\n",
    "# L1Unstructured(l1_model, l1_params, .2)\n",
    "# RandUnstructured(unstruct_model, unstruct_params, .2)\n",
    "\n",
    "# l1_shap, l1_lime = SHAP(l1_model, tokenizer, device), LIME(l1_model, tokenizer, device)\n",
    "# l1_shap_evaluator, l1_lime_evaluator = Evaluator(l1_shap), Evaluator(l1_lime)\n",
    "\n",
    "\n",
    "\n",
    "inputs = ['Camilo CANNOT CODE FOR HIS LIFE. I DONT LIKE HIM!!!!',\n",
    "          'David is GREAT at soccer. Can recommend <thumbs up>!',\n",
    "          'Joey is joey. I feel very neutrally about him',\n",
    "          'Finale is the best professor Harvard has EVER had. Would recommend!',\n",
    "          'I AM GOING TO SCREAMMMMMMMMMMM AHHHHHHHHHHHH',\n",
    "          'Paula and Hiwot are great TFs!']\n",
    "\n",
    "# l1_shap_evaluator.get_local_infidelity(inputs[0])\n",
    "# l1_lime_evaluator.get_local_infidelity(inputs[0])\n",
    "infidelities = eval_suite(model, tokenizer, inputs, .20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unpruned': {'shap': [16.697126388549805,\n",
       "   0.06618619710206985,\n",
       "   0.47682851552963257,\n",
       "   0.15279464423656464,\n",
       "   25.01079750061035,\n",
       "   0.16142603754997253]},\n",
       " 'l1unstruct': {'shap': [16.697126388549805,\n",
       "   0.06618619710206985,\n",
       "   0.47682851552963257,\n",
       "   0.15279464423656464,\n",
       "   25.01079750061035,\n",
       "   0.16142603754997253]},\n",
       " 'randunstruct': {'shap': [16.697126388549805,\n",
       "   0.06618619710206985,\n",
       "   0.47682851552963257,\n",
       "   0.15279464423656464,\n",
       "   25.01079750061035,\n",
       "   0.16142603754997253]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infidelities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2822",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
