{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruners.pruning_methods import L1Unstructured, RandUnstructured\n",
    "import torch\n",
    "from evaluations.evaluator import Evaluator\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from explainers.explanation_methods import SHAP, LIME\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of model params given a bert `model`\n",
    "def get_params_to_prune(model):\n",
    "    params_to_prune = []\n",
    "    for layer in model.bert.encoder.layer:\n",
    "        # Attention weights (query, key, value, and output projection)\n",
    "        params_to_prune.append((layer.attention.self.query, 'weight'))\n",
    "        params_to_prune.append((layer.attention.self.key, 'weight'))\n",
    "        params_to_prune.append((layer.attention.self.value, 'weight'))\n",
    "        params_to_prune.append((layer.attention.output.dense, 'weight'))\n",
    "        \n",
    "        # Intermediate dense layer\n",
    "        params_to_prune.append((layer.intermediate.dense, 'weight'))\n",
    "        \n",
    "        # Output dense layer\n",
    "        params_to_prune.append((layer.output.dense, 'weight'))\n",
    "        \n",
    "    return params_to_prune\n",
    "\n",
    "\n",
    "def eval_suite(model, tokenizer, inputs, prune_ptg):\n",
    "    # TODO: Refactor\n",
    "    \n",
    "    model.to(device)\n",
    "    # Init models and grab model params to prune\n",
    "    randunstructured_model = deepcopy(model).to(device)\n",
    "    l1unstructured_model = deepcopy(model).to(device)\n",
    "\n",
    "    randunstruct_params = get_params_to_prune(randunstructured_model)\n",
    "    l1unstruct_params = get_params_to_prune(l1unstructured_model)\n",
    "\n",
    "    # Initialize pruners and make pruned models\n",
    "    print('Pruning models...')\n",
    "    unpruned_model = model\n",
    "    \n",
    "    randunstructured_pruner = RandUnstructured()\n",
    "    l1unstructured_pruner = L1Unstructured()\n",
    "    randunstructured_pruner.prune(randunstruct_params, prune_ptg)\n",
    "    l1unstructured_pruner.prune(l1unstruct_params, prune_ptg)\n",
    "\n",
    "    # Init explainers\n",
    "    shap_randunstruct, lime_randunstruct = SHAP(randunstructured_model, tokenizer, device),\\\n",
    "                                           LIME(randunstructured_model, tokenizer, device)\n",
    "    shap_l1unstruct, lime_l1unstruct = SHAP(l1unstructured_model, tokenizer, device),\\\n",
    "                                       LIME(l1unstructured_model, tokenizer, device)\n",
    "    shap_unpruned, lime_unpruned = SHAP(unpruned_model, tokenizer, device),\\\n",
    "                                   LIME(unpruned_model, tokenizer, device)\n",
    "    \n",
    "    # Init evaluators\n",
    "    randunstruct_evaluators = {'shap': Evaluator(shap_randunstruct)}\n",
    "                               #'lime': Evaluator(lime_randunstruct)}\n",
    "    l1unstruct_evaluators = {'shap': Evaluator(shap_l1unstruct)}\n",
    "                            # 'lime': Evaluator(lime_l1unstruct)}\n",
    "    unrpuned_evaluators = {'shap': Evaluator(shap_unpruned)}\n",
    "                           #'lime': Evaluator(lime_unpruned)}\n",
    "\n",
    "    # Janky for now but hang with me\n",
    "    infidelities_ = {'unpruned': unrpuned_evaluators,\n",
    "                    'l1unstruct': l1unstruct_evaluators,\n",
    "                    'randunstruct': randunstruct_evaluators}\n",
    "    \n",
    "    # Run evaluations, storing in dictionary of \n",
    "    # {prune_method: \n",
    "    #   {explanation_method: infidelity}\n",
    "    # }\n",
    "    print('Evaluating explanations...')\n",
    "    infidelities = {}\n",
    "    for input in tqdm(inputs, desc='Evaluating', unit='input'):\n",
    "        for prune_method, evaluator_set in infidelities_.items():\n",
    "            # Initialize prune_method in infidelities if not present\n",
    "            if prune_method not in infidelities:\n",
    "                infidelities[prune_method] = {}  # Initialize prune_method dict\n",
    "            \n",
    "            for expla_method, evaluator in evaluator_set.items():\n",
    "                # Initialize expla_method as a list if not present\n",
    "                if expla_method not in infidelities[prune_method]:\n",
    "                    infidelities[prune_method][expla_method] = []\n",
    "                \n",
    "                # Append the result of get_local_infidelity to the list\n",
    "                infidelities[prune_method][expla_method].append(evaluator.get_local_infidelity(input))\n",
    "\n",
    "    return infidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-yelp-polarity\")\n",
    "model = model.to(device)\n",
    "\n",
    "inputs = ['Camilo CANNOT CODE FOR HIS LIFE. I DONT LIKE HIM!!!!',\n",
    "          'David is GREAT at soccer. Can recommend <thumbs up>!',\n",
    "          'Joey is joey. I feel very neutrally about him',\n",
    "          'Finale is the best professor Harvard has EVER had. Would recommend!',\n",
    "          'I AM GOING TO SCREAMMMMMMMMMMM AHHHHHHHHHHHH',\n",
    "          'Paula and Hiwot are great TFs!']\n",
    "\n",
    "infidelities = eval_suite(model, tokenizer, inputs, .20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infidelities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
